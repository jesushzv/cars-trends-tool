# Phase 19.6: Automatic Scheduling & Data Retention

## âœ… Completed

**Goal**: Implement fully automated daily scraping with historical data tracking and retention policies.

## ğŸ¯ Key Features Implemented

### 1. Listing Lifecycle Tracking
**Problem**: Previously, duplicate listings were skipped entirely, losing historical information.

**Solution**: Implemented lifecycle tracking with `first_seen` and `last_seen` timestamps.

**Changes**:
- **`models.py`**: Added `first_seen` and `last_seen` columns to `Listing` model
- **`migrate_add_lifecycle.py`**: Migration script to add columns to existing databases
- **`services/listing_lifecycle_service.py`**: New service for upsert logic
  - `upsert_listing()`: Create or update listings based on URL
  - Tracks price changes over time
  - Updates engagement metrics on subsequent scrapes
- **`db_service.py`**: Updated to use upsert instead of skip-on-duplicate

**Behavior**:
```
Day 1: Find Honda Civic at $15,000 â†’ first_seen = last_seen = Day 1
Day 2: Still listed at $15,000 â†’ last_seen = Day 2 (first_seen unchanged)
Day 3: Price drops to $14,500 â†’ last_seen = Day 3, price updated, logged
Day 91: Cleanup runs â†’ Remove if not seen in 90 days
```

### 2. Data Retention & Cleanup
**Problem**: Without cleanup, database would grow indefinitely.

**Solution**: Automated cleanup service with configurable retention periods.

**Changes**:
- **`services/cleanup_service.py`**: New service for data retention
  - `cleanup_old_listings()`: Remove listings last seen > X days ago
  - `cleanup_old_snapshots()`: Remove snapshots > Y days old
  - `cleanup_all()`: Run all cleanup operations
  - `get_cleanup_stats()`: View data age statistics

**Configuration** (Environment variables):
- `LISTING_RETENTION_DAYS=90` (default)
- `SNAPSHOT_RETENTION_DAYS=180` (default, 2x listings)

**Schedule**: Daily at 6:00 AM (Tijuana time)

### 3. Automatic Initial Data Seeding
**Problem**: New deployments showed empty dashboard until first scrape.

**Solution**: Automatic data seeding on first startup.

**Changes**:
- **`seed_data.py`**: Initial data population script
  - Checks if database is empty
  - Runs full scrape of all platforms if needed
  - Creates initial daily snapshot
  - Fails gracefully if platforms unavailable
  - **Special handling**: Facebook failures trigger alerting (per user requirement)

**Behavior**:
```
First startup â†’ Database empty â†’ Seed with initial scrape â†’ Users see data immediately
Subsequent startups â†’ Database has data â†’ Skip seeding â†’ Continue normal operation
```

### 4. Automatic Scheduler Startup
**Problem**: Manual scheduler start/stop was required.

**Solution**: Scheduler auto-starts on application launch.

**Changes**:
- **`services/scheduler_service.py`**: Enhanced scheduler
  - Added `_cleanup_job()` function for daily cleanup
  - Added `_facebook_failure_alert()` for alerting (per user requirement)
  - Updated `initialize_scheduler(auto_start=True)` to auto-start by default
  - Now schedules 5 jobs:
    1. 2:00 AM - Scrape Craigslist
    2. 3:00 AM - Scrape Mercado Libre
    3. 4:00 AM - Scrape Facebook (with alerting on failure)
    4. 5:00 AM - Create Daily Snapshot
    5. 6:00 AM - Cleanup Old Data (NEW)

- **`main.py`**: Enhanced startup event
  ```python
  @app.on_event("startup")
  def startup_event():
      1. Create database tables
      2. Seed initial data if empty
      3. Initialize and auto-start scheduler
  ```

### 5. Frontend Updates
**Problem**: Manual scheduler controls no longer needed/wanted.

**Solution**: Removed controls, kept read-only status display.

**Changes**:
- **`frontend/index.html`**:
  - âŒ Removed: Start/Stop/Refresh buttons
  - âŒ Removed: `startScheduler()` and `stopScheduler()` functions
  - âœ… Kept: Status display (read-only)
  - âœ… Added: Informative message "Scraping runs automatically every day at 2-4 AM"
  - âœ… Updated: Status shows next scheduled job

**User Experience**:
- Users see scheduler is "Active" with next run time
- No manual intervention needed or possible
- Clear expectation: data updates automatically overnight

## ğŸ“Š Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Application Startup                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Create tables                       â”‚
â”‚  2. Check if DB empty                   â”‚
â”‚  3. If empty â†’ Seed initial data        â”‚
â”‚  4. Initialize scheduler (auto-start)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Daily Operations (Automated)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2 AM: Scrape Craigslist               â”‚
â”‚  3 AM: Scrape Mercado Libre            â”‚
â”‚  4 AM: Scrape Facebook (alert on fail) â”‚
â”‚  5 AM: Create daily snapshot           â”‚
â”‚  6 AM: Cleanup old data                â”‚
â”‚                                         â”‚
â”‚  For each listing:                      â”‚
â”‚    - URL exists? â†’ Update last_seen    â”‚
â”‚    - URL new? â†’ Create with timestamps â”‚
â”‚    - Price changed? â†’ Log & update     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Data Retention                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Listings: Keep 90 days                â”‚
â”‚  Snapshots: Keep 180 days              â”‚
â”‚  Cleanup: Runs daily at 6 AM           â”‚
â”‚  Database: Size stays bounded          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—„ï¸ Database Schema Changes

### Listing Table (Updated)
```sql
CREATE TABLE listings (
    -- Existing columns...
    first_seen TIMESTAMP,      -- When first discovered (NEW)
    last_seen TIMESTAMP,       -- When last seen active (NEW)
    scraped_at TIMESTAMP       -- Deprecated, kept for compatibility
);
```

## ğŸ” Key Decisions

| Decision | Option Chosen | Rationale |
|----------|---------------|-----------|
| **Duplicate Handling** | Option B: Lifecycle tracking | Track listing lifetime and price changes |
| **Listing Retention** | 90 days | Quarterly trends, industry standard |
| **Snapshot Retention** | 180 days (2x listings) | Snapshots are small, valuable for long-term trends |
| **Cleanup Schedule** | Daily at 6 AM | Automated, runs when traffic is low |
| **Initial Seeding** | Single scrape on first deploy | Immediate value for users |
| **Scraping Frequency** | Once daily | Perfect for market trend analysis |
| **Facebook Failures** | Fail gracefully + alerting | Comprehensive logging for troubleshooting |
| **Scheduler Controls** | Remove (read-only status only) | Fully automated, no manual intervention |

## ğŸ“ Configuration

Add to `.env`:
```bash
# Data retention (optional, defaults shown)
LISTING_RETENTION_DAYS=90
SNAPSHOT_RETENTION_DAYS=180
```

## ğŸ§ª Testing

Run migration:
```bash
cd backend
python migrate_add_lifecycle.py
```

Test lifecycle service:
```bash
python services/listing_lifecycle_service.py
```

Test cleanup service:
```bash
python services/cleanup_service.py
```

Test seed script:
```bash
python seed_data.py
```

Start application (auto-seeds and auto-starts scheduler):
```bash
uvicorn main:app --reload
```

## ğŸ“š API Endpoints

### Unchanged (Still Available)
- `GET /scheduler/status` - View scheduler status (read-only)

### Removed
- ~~`POST /scheduler/start`~~ - No longer needed (auto-starts)
- ~~`POST /scheduler/stop`~~ - No longer needed (always running)

## âœ… Success Criteria

- [x] ~~Listings table~~ â†’ Has `first_seen` and `last_seen` columns
- [x] Scrapers â†’ Use upsert logic (update existing URLs)
- [x] Cleanup service â†’ Removes old data automatically
- [x] Seed script â†’ Populates empty database
- [x] Scheduler â†’ Auto-starts on app launch
- [x] Scheduler â†’ Includes cleanup job at 6 AM
- [x] Frontend â†’ Shows read-only status (no controls)
- [x] Facebook failures â†’ Trigger comprehensive alerting
- [x] Database size â†’ Stays bounded (90-day retention)
- [x] Users â†’ See data immediately on fresh deploys

## ğŸ¯ User Experience

### Before Phase 19.6
1. Deploy application
2. Dashboard is empty
3. Manually start scheduler
4. Wait for first scrape
5. Duplicates skipped (no history)
6. Database grows forever

### After Phase 19.6
1. Deploy application
2. **Dashboard has data immediately** (auto-seeded)
3. **Scheduler already running** (auto-started)
4. Daily updates happen automatically
5. **Historical data tracked** (lifecycle)
6. **Database size managed** (cleanup)
7. No manual intervention needed

## ğŸš€ Next Phase Suggestions

1. **Cloud Deployment** (Phase 20)
   - Deploy to cloud platform
   - Set up production database
   - Configure production secrets
   
2. **Monitoring & Alerts** (Future Phase)
   - Email alerts for Facebook failures
   - Dashboard health checks
   - Scraping success metrics
   
3. **Advanced Analytics** (Future Phase)
   - Days on market analysis
   - Price change alerts
   - Sold listings tracking

## ğŸ“– Files Created/Modified

### New Files
- `backend/services/listing_lifecycle_service.py`
- `backend/services/cleanup_service.py`
- `backend/seed_data.py`
- `backend/migrate_add_lifecycle.py`
- `PHASE_19.6_AUTO_SCHEDULING_REVISED_DESIGN.md`
- `PHASE_19.6_SUMMARY.md` (this file)

### Modified Files
- `backend/models.py` - Added lifecycle columns
- `backend/db_service.py` - Use upsert logic
- `backend/services/scheduler_service.py` - Auto-start, cleanup job, alerting
- `backend/main.py` - Enhanced startup event
- `frontend/index.html` - Removed controls, updated status display

### Configuration
- `.env` - Add `LISTING_RETENTION_DAYS`, `SNAPSHOT_RETENTION_DAYS`

## ğŸ‰ Phase 19.6 Complete!

All requirements implemented and tested. The application now:
- âœ… Runs fully automated daily scraping
- âœ… Preserves historical data with lifecycle tracking
- âœ… Manages database size with retention policies
- âœ… Seeds initial data on first deployment
- âœ… Auto-starts scheduler (no manual control needed)
- âœ… Alerts on Facebook scraping failures
- âœ… Provides clear user experience

**Ready for cloud deployment (Phase 20)!** ğŸš€

